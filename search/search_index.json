{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COMP4DRONES components repository Welcome to the COMP4DRONES component repository. To know more about COMP4DRONES please visit comp4drones.eu . This repository aims at providing common components usable in different application domains, in particular those covered by project use-cases. The requirements for using a components will be listed, as well as a documentation on how to use it. The component itself will be hosted by the partner who provides it.","title":"Home"},{"location":"#comp4drones-components-repository","text":"Welcome to the COMP4DRONES component repository. To know more about COMP4DRONES please visit comp4drones.eu . This repository aims at providing common components usable in different application domains, in particular those covered by project use-cases. The requirements for using a components will be listed, as well as a documentation on how to use it. The component itself will be hosted by the partner who provides it.","title":"COMP4DRONES components repository"},{"location":"about/","text":"COMP4DRONES is an ECSEL JU project coordinated by Indra that brings together a consortium of 49 partners with the aim of providing a framework of key enabling technologies for safe and autonomous drones. It brings to bear a holistically designed ecosystem from application to electronic components, realized as a tightly integrated multi-vendor and compositional UAV embedded architecture solution and a tool chain complementing the compositional architecture principles. The project will mainly focus on the following objectives: Ease the integration and customization of embedded drone systems. Enable drones to take safe autonomous decisions . Ensure the deployment of trusted communications . Minimize the design and verification effort for complex drone applications. Ensuring sustainable impact and creation of an industry-driven community. Demonstration and validation activities are essential to ensure the quality and relevance of innovations. COMP4DRONES will ease the development of new application and functionalities on the fields of transport, construction, surveillance and inspection, logistics, and agriculture","title":"About"},{"location":"wp3_components_list/","text":"Components list ID Contributor Title WP3-01 IKERLAN Safety function - Pre-Certified SOM WP3-02 EDI Modular SoC-based embedded reference architecture WP3-03 BUT Sensor information algorithms WP3-04 HIB Computer Vision Components for drones WP3-07 TNL Robust Communication WP3-08 TUE A compact, lightweight platform (MPSOC FPGA-based, incl. RTOS) WP3-10 IFAT Component for trusted communication WP3-13 ENAC Paparazzi UAV WP3-14_1 ENSMA Control components that implement potential barriers WP3-14_2 ENSMA Multi-agent swarm control WP3-15_1, WP4-17, WP5-19 ACORDE UWB based indoor positioning WP3-15_2, WP4-16, WP5-11 ACORDE Multi-antenna GNSS/INS based navigation WP3-16 SCALIAN EZ_Chains Fleet Architecture WP3-19_1 IMEC Hyperspectral payload WP3-19_2 IMEC Hyperspectral image processing WP3-20 MODIS Multi-sensor positioning WP3-22 UNIMORE Onboard Compute Platform Desing Methodology WP3-24 UNIVAQ Efficient digital implementation of controllers WP3-26 UWB Droneport: an autonomous drone battery management system WP3-27 INDRA_1 Visible and Infrared HD camera WP3-28 UNISS Onboard Compute Platform Design Paradigm WP3-30 FADA-CATEC Multi-sensor positioning WP3-32_1 CEA TSN Queue Mapper WP3-32_2 CEA_2 Reference Architecture Modelling and Code Generation WP3-36_1 UDANET Smart and predictive energy management system WP3-36_2 UDANET AI drone system modules WP4-2 SCALIAN EZ_Land Precision landing WP4-5 SCALIAN AI detection for clearance WP4-42 SCALIAN AI Stabilization WP5-03 SCALIAN EZ_Com Safe fleet communication","title":"Components list"},{"location":"wp3_components_list/#components-list","text":"ID Contributor Title WP3-01 IKERLAN Safety function - Pre-Certified SOM WP3-02 EDI Modular SoC-based embedded reference architecture WP3-03 BUT Sensor information algorithms WP3-04 HIB Computer Vision Components for drones WP3-07 TNL Robust Communication WP3-08 TUE A compact, lightweight platform (MPSOC FPGA-based, incl. RTOS) WP3-10 IFAT Component for trusted communication WP3-13 ENAC Paparazzi UAV WP3-14_1 ENSMA Control components that implement potential barriers WP3-14_2 ENSMA Multi-agent swarm control WP3-15_1, WP4-17, WP5-19 ACORDE UWB based indoor positioning WP3-15_2, WP4-16, WP5-11 ACORDE Multi-antenna GNSS/INS based navigation WP3-16 SCALIAN EZ_Chains Fleet Architecture WP3-19_1 IMEC Hyperspectral payload WP3-19_2 IMEC Hyperspectral image processing WP3-20 MODIS Multi-sensor positioning WP3-22 UNIMORE Onboard Compute Platform Desing Methodology WP3-24 UNIVAQ Efficient digital implementation of controllers WP3-26 UWB Droneport: an autonomous drone battery management system WP3-27 INDRA_1 Visible and Infrared HD camera WP3-28 UNISS Onboard Compute Platform Design Paradigm WP3-30 FADA-CATEC Multi-sensor positioning WP3-32_1 CEA TSN Queue Mapper WP3-32_2 CEA_2 Reference Architecture Modelling and Code Generation WP3-36_1 UDANET Smart and predictive energy management system WP3-36_2 UDANET AI drone system modules WP4-2 SCALIAN EZ_Land Precision landing WP4-5 SCALIAN AI detection for clearance WP4-42 SCALIAN AI Stabilization WP5-03 SCALIAN EZ_Com Safe fleet communication","title":"Components list"},{"location":"components/Complete_UAS/ENAC_paparazzi/","text":"Paparazzi UAV ID: WP3-13 Contributor: ENAC Owner: Licence: GPL expected TRL: 5 KET: Intelligent mission management, Intelligent outer loop control, Take-off, Landing, Planning and scheduling, Fail-safe mission Contact: fabien.bonneval@enac.fr Description Paparazzi is a complete system of open source hardware and software for Unmanned Aircraft Systems (UAS), including both the airborne autopilot as well as complete ground station mission planning and monitoring software utilizing a bi-directional datalink for telemetry and control. Paparazzi has been created at ENAC in 2003, and is now supported by other institutes such as MAVLAB of the TU-Delft, individual developers, and some private UAV companies from several countries. The Paparazzi system was initially designed for robust small fixed-wing aircrafts in 2003, but it now support several other configurations and concepts such as high-aspect ratio gliders, multi-rotors, transitioning vehicles, and rovers. Today, Paparazzi flies on 16cm Quark up to 4.3 meter spanned Adler UAV from University of Stuttgart. The popular UAV platform Bebop2 from Parrot2 can also be used to run the Paparazzi software. See the Wiki and the Github repository . Current state Paparazzi is a running, working project, but it lacks some tools to make it more stable. Improvements The methods coming from the WP6 will be applied to the paparazzi architecture and its modules. Standards and use case from WP2 and WP1 will also be taken into account. Overview Figure 1 The global view of the system (see figure 1) is very typical of UAV systems: Ground segment: all the ground software and hardware infrastructures that are used to prepare, monitor and analyze the flight. Airborne segment: the aircraft, its hardware parts including payload and all the embedded software to control the flight (from stabilization to decision making). Communication segment: defines all the communication links and protocols between the ground and airborne segments. Safety link: safety remote control that can be separated from the ground station for short range direct control. System architecture Figure 2 The Paparazzi system architecture is show in Figure 3. The UAV (in blue) is navigating autonomously and is monitored and controlled from the ground (in brown). The ground control station (GCS), or GCS agent, provides a graphical user interface with telemetry data received by the link agent which manages the ground-based radio modem. The link agent distributes telemetry data across the network (a single computer, a local network or the internet) where it can be used locally or remotely by the: (1) server - an agent that logs, distributes, and preprocesses these messages for the GCS and other agents; (2) messages - a real-time numeric display of all telemetry data; (3) a number of other useful agents. These agents are: a GCS-based flight plan editor to modify waypoints a UAV simulator to test flight plans and code modifications a real-time plotter for graphical telemetry data visualization a log plotter for graphical telemetry visualization after a flight All of these processes run simultaneously and each module is independently launched and can be configured via the Paparazzi Center. Communication with the autopilot is based on a custom protocol and message definition, called Pprzlink. Libraries exist for airborne and ground segments, in multiple languages (C, OCaml, Python), and for multiple transport protocols (Serial, UDP, Ivy). On the ground segment, ground control station uses Pprzlink with the Ivy transport. Ivy is a software publish/subscribe bus based on UDP or TCP to exchange text messages. There are four ways to extend Paparazzi functionalities: Using Pprzlink with the current ground station tools Using Pprzlink independently of the ground station tools Adding a dedicated board on the drone and using Pprzlink to communicate directly with the autopilot (usually with the serial transport) Adding a module to the autopilot itself A combination of these options can be used at the same time to achieve specific goals. Using Pprzlink with the ground station The easier way to interact with Paparazzi is to create a new agent as part of the ground station. All parameters and configuration files are easily accessible, making it easy to interact with the drone or display new data on the GCS. As example, the Interactive Informatics team of ENAC used it to design novel human-drone interactions for safety pilots and adaptable interactions for pilots with disabilities. Using Pprzlink without the ground station It is also possible to use Pprzlink on the ground by interacting directly with the datalink. This approach gives less high-level services but makes it simpler to exchange custom messages. Adding a dedicated board on the drone Custom dedicated boards (Raspberry or Arduino for instance) can be integrated in the drone and interact with the autopilot through a local link, typically a serial interface between the dedicated board and the autopilot. The type of interaction is the same than with a ground agent. Adding a module to the autopilot itself The modularity of the airborne architecture (see Figure 3) also enables users to write custom modules that will run on the autopilot itself. All sensors data, payload or actuators are then available to this module. Modules are the building blocks of the airborne architecture. These modules can do various things such as handling a specific sensor connected to the autopilot, enhance navigation capabilities, implement custom stabilization or guidance control loops... Figure 3 Paparazzi also has a simulator based on JSBSim . The autopilot can be built for both the hardware target and the simulator target. The simulator then uses the same code that will run on the real hardware. Custom modules that use platform specific capabilities must provide alternative code in order to be built for another target, such as the simulator.","title":"Paparazzi"},{"location":"components/Complete_UAS/ENAC_paparazzi/#paparazzi-uav","text":"ID: WP3-13 Contributor: ENAC Owner: Licence: GPL expected TRL: 5 KET: Intelligent mission management, Intelligent outer loop control, Take-off, Landing, Planning and scheduling, Fail-safe mission Contact: fabien.bonneval@enac.fr","title":"Paparazzi UAV"},{"location":"components/Complete_UAS/ENAC_paparazzi/#description","text":"Paparazzi is a complete system of open source hardware and software for Unmanned Aircraft Systems (UAS), including both the airborne autopilot as well as complete ground station mission planning and monitoring software utilizing a bi-directional datalink for telemetry and control. Paparazzi has been created at ENAC in 2003, and is now supported by other institutes such as MAVLAB of the TU-Delft, individual developers, and some private UAV companies from several countries. The Paparazzi system was initially designed for robust small fixed-wing aircrafts in 2003, but it now support several other configurations and concepts such as high-aspect ratio gliders, multi-rotors, transitioning vehicles, and rovers. Today, Paparazzi flies on 16cm Quark up to 4.3 meter spanned Adler UAV from University of Stuttgart. The popular UAV platform Bebop2 from Parrot2 can also be used to run the Paparazzi software. See the Wiki and the Github repository .","title":"Description"},{"location":"components/Complete_UAS/ENAC_paparazzi/#current-state","text":"Paparazzi is a running, working project, but it lacks some tools to make it more stable.","title":"Current state"},{"location":"components/Complete_UAS/ENAC_paparazzi/#improvements","text":"The methods coming from the WP6 will be applied to the paparazzi architecture and its modules. Standards and use case from WP2 and WP1 will also be taken into account.","title":"Improvements"},{"location":"components/Complete_UAS/ENAC_paparazzi/#overview","text":"Figure 1 The global view of the system (see figure 1) is very typical of UAV systems: Ground segment: all the ground software and hardware infrastructures that are used to prepare, monitor and analyze the flight. Airborne segment: the aircraft, its hardware parts including payload and all the embedded software to control the flight (from stabilization to decision making). Communication segment: defines all the communication links and protocols between the ground and airborne segments. Safety link: safety remote control that can be separated from the ground station for short range direct control.","title":"Overview"},{"location":"components/Complete_UAS/ENAC_paparazzi/#system-architecture","text":"Figure 2 The Paparazzi system architecture is show in Figure 3. The UAV (in blue) is navigating autonomously and is monitored and controlled from the ground (in brown). The ground control station (GCS), or GCS agent, provides a graphical user interface with telemetry data received by the link agent which manages the ground-based radio modem. The link agent distributes telemetry data across the network (a single computer, a local network or the internet) where it can be used locally or remotely by the: (1) server - an agent that logs, distributes, and preprocesses these messages for the GCS and other agents; (2) messages - a real-time numeric display of all telemetry data; (3) a number of other useful agents. These agents are: a GCS-based flight plan editor to modify waypoints a UAV simulator to test flight plans and code modifications a real-time plotter for graphical telemetry data visualization a log plotter for graphical telemetry visualization after a flight All of these processes run simultaneously and each module is independently launched and can be configured via the Paparazzi Center. Communication with the autopilot is based on a custom protocol and message definition, called Pprzlink. Libraries exist for airborne and ground segments, in multiple languages (C, OCaml, Python), and for multiple transport protocols (Serial, UDP, Ivy). On the ground segment, ground control station uses Pprzlink with the Ivy transport. Ivy is a software publish/subscribe bus based on UDP or TCP to exchange text messages. There are four ways to extend Paparazzi functionalities: Using Pprzlink with the current ground station tools Using Pprzlink independently of the ground station tools Adding a dedicated board on the drone and using Pprzlink to communicate directly with the autopilot (usually with the serial transport) Adding a module to the autopilot itself A combination of these options can be used at the same time to achieve specific goals.","title":"System architecture"},{"location":"components/Complete_UAS/ENAC_paparazzi/#using-pprzlink-with-the-ground-station","text":"The easier way to interact with Paparazzi is to create a new agent as part of the ground station. All parameters and configuration files are easily accessible, making it easy to interact with the drone or display new data on the GCS. As example, the Interactive Informatics team of ENAC used it to design novel human-drone interactions for safety pilots and adaptable interactions for pilots with disabilities.","title":"Using Pprzlink with the ground station"},{"location":"components/Complete_UAS/ENAC_paparazzi/#using-pprzlink-without-the-ground-station","text":"It is also possible to use Pprzlink on the ground by interacting directly with the datalink. This approach gives less high-level services but makes it simpler to exchange custom messages.","title":"Using Pprzlink without the ground station"},{"location":"components/Complete_UAS/ENAC_paparazzi/#adding-a-dedicated-board-on-the-drone","text":"Custom dedicated boards (Raspberry or Arduino for instance) can be integrated in the drone and interact with the autopilot through a local link, typically a serial interface between the dedicated board and the autopilot. The type of interaction is the same than with a ground agent.","title":"Adding a dedicated board on the drone"},{"location":"components/Complete_UAS/ENAC_paparazzi/#adding-a-module-to-the-autopilot-itself","text":"The modularity of the airborne architecture (see Figure 3) also enables users to write custom modules that will run on the autopilot itself. All sensors data, payload or actuators are then available to this module. Modules are the building blocks of the airborne architecture. These modules can do various things such as handling a specific sensor connected to the autopilot, enhance navigation capabilities, implement custom stabilization or guidance control loops... Figure 3 Paparazzi also has a simulator based on JSBSim . The autopilot can be built for both the hardware target and the simulator target. The simulator then uses the same code that will run on the real hardware. Custom modules that use platform specific capabilities must provide alternative code in order to be built for another target, such as the simulator.","title":"Adding a module to the autopilot itself"},{"location":"components/Complete_UAS/SCALIAN-ez_chains/","text":"EZ_Chains Fleet Architecture ID: WP3-16 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 8 KET: 1.1.7 Operations management, 2.1.1 Intelligent Mission Management, 2.2.3 Planning and Scheduling, 2.2.4 Fail-safe Mission, 2.5.1 Drone and Rover, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com EZ_Chains is an architecture designed for fleet of UAS so they can autonomously perform cooperativ tasks to achieve complex operations. EZ_Chains is a generic architecutre that combines different components to achieve autonomous fleet behaviour. This genericty allows to use heterogeneous UAS and alloxate them different missions. For instance, EZ_Chains is used in the Metis project for the UC3 D1: the goal is to use a fleet of five UAS to autonmoously drop seismic sensor over a large area. In addition to the dropping UAS, a surveillance UAS is used to detect potential intruders on the operation areas, preventing dropping operations to increase the safety of the system. The figure below demonstrates the main category of components that build EZ_Chains: On a more technical aspect, EZ_Chains relies on a centralised knowledge base that allows distributed (embedded) decision making. The architecture is composed of hierarchical components: there is a decision layer to provide plans to the monitoring/execution layer that uses the interface layer to control the robot and its payload. The modules composing EZ_Chains: The knowledge base provides a view of the fleet situation and the mission evolution in a SQL database. It also triggers notifications for each fleet/mission event. Through its API, each agent of the fleet (UAV, GCS, C&C, etc.) is able to declare critical information, to book shared resources, to send orders or instructions to other agents, etc. It uses the communication system to ensure that the state is correctly synchronized among the fleet. The task monitor is the central node of EZ_Chains. This module sends requests to all the other modules in order to perform actions. The task monitor communicates its planned moves and actions to the fleet by sending transactions to the knowledge base that are transmitted to the fleet thanks to the communication system. The task monitor is also listening to the fleet/mission events triggered by the knowledge base in order to detect conflicts with its initial plan. The task planner receives request from the task monitor to compute the best plan (that is, the best sequence of actions) in order to achieve a goal (e.g. drop six DARTs). To do its computation, the task planner reads the knowledge base to get the current status of the mission. The navigation monitor is in charge of the UAV navigation. It complies with the task monitor queries (e.g. take-off, move to, hold position, etc.). This module ensures the safety of the UAV by doing air traffic management, collision detection, and forbidden areas avoidance. The navigation path planner is used by the navigation monitor to compute an efficient path to reach a destination, avoiding the forbidden areas, using the corridors if needed and avoiding potential conflicts with the path of other UAVs. The autopilot interface provides the low level services to control the UAV behavior. It receives the instructions from the navigation monitor and reports the UAV status to it. In the figure, two payload interfaces are depicted that are related to the mission carried out by the system. There are used to accomplish the Logistics use-case (UC3 \u2013 Demo 1), where sensors have to be dropped. Hence there are three sub-modules: The camera interface provides the low level services to control the camera behavior and its gimbal. It receives the commands from the task monitor. The cassette monitor is running on the cassette controller and is connected to the task monitor through a network connection between the cassette and the drone. On a dropper UAV, the cassette provides services related to the dropping of DARTs, but any kind of payload could be handled. The cassette interface provides the low level services to manage the cassette sensors and actuators. It receives the instructions from the cassette monitor and reports the cassette status to it. The ground version of 'EZ_Chains', also called black box embeds the communication system and the knowledge to allow the GCS and the C&C to define the mission and to interact with the fleet. EZ_Chains ground also embeds a tiny mission monitor module doing some checks about the fleet and the mission evolution.","title":"SCALIAN \u2013 EZ_Chains Fleet Architecture"},{"location":"components/Complete_UAS/SCALIAN-ez_chains/#ez_chains-fleet-architecture","text":"ID: WP3-16 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 8 KET: 1.1.7 Operations management, 2.1.1 Intelligent Mission Management, 2.2.3 Planning and Scheduling, 2.2.4 Fail-safe Mission, 2.5.1 Drone and Rover, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com EZ_Chains is an architecture designed for fleet of UAS so they can autonomously perform cooperativ tasks to achieve complex operations. EZ_Chains is a generic architecutre that combines different components to achieve autonomous fleet behaviour. This genericty allows to use heterogeneous UAS and alloxate them different missions. For instance, EZ_Chains is used in the Metis project for the UC3 D1: the goal is to use a fleet of five UAS to autonmoously drop seismic sensor over a large area. In addition to the dropping UAS, a surveillance UAS is used to detect potential intruders on the operation areas, preventing dropping operations to increase the safety of the system. The figure below demonstrates the main category of components that build EZ_Chains: On a more technical aspect, EZ_Chains relies on a centralised knowledge base that allows distributed (embedded) decision making. The architecture is composed of hierarchical components: there is a decision layer to provide plans to the monitoring/execution layer that uses the interface layer to control the robot and its payload. The modules composing EZ_Chains: The knowledge base provides a view of the fleet situation and the mission evolution in a SQL database. It also triggers notifications for each fleet/mission event. Through its API, each agent of the fleet (UAV, GCS, C&C, etc.) is able to declare critical information, to book shared resources, to send orders or instructions to other agents, etc. It uses the communication system to ensure that the state is correctly synchronized among the fleet. The task monitor is the central node of EZ_Chains. This module sends requests to all the other modules in order to perform actions. The task monitor communicates its planned moves and actions to the fleet by sending transactions to the knowledge base that are transmitted to the fleet thanks to the communication system. The task monitor is also listening to the fleet/mission events triggered by the knowledge base in order to detect conflicts with its initial plan. The task planner receives request from the task monitor to compute the best plan (that is, the best sequence of actions) in order to achieve a goal (e.g. drop six DARTs). To do its computation, the task planner reads the knowledge base to get the current status of the mission. The navigation monitor is in charge of the UAV navigation. It complies with the task monitor queries (e.g. take-off, move to, hold position, etc.). This module ensures the safety of the UAV by doing air traffic management, collision detection, and forbidden areas avoidance. The navigation path planner is used by the navigation monitor to compute an efficient path to reach a destination, avoiding the forbidden areas, using the corridors if needed and avoiding potential conflicts with the path of other UAVs. The autopilot interface provides the low level services to control the UAV behavior. It receives the instructions from the navigation monitor and reports the UAV status to it. In the figure, two payload interfaces are depicted that are related to the mission carried out by the system. There are used to accomplish the Logistics use-case (UC3 \u2013 Demo 1), where sensors have to be dropped. Hence there are three sub-modules: The camera interface provides the low level services to control the camera behavior and its gimbal. It receives the commands from the task monitor. The cassette monitor is running on the cassette controller and is connected to the task monitor through a network connection between the cassette and the drone. On a dropper UAV, the cassette provides services related to the dropping of DARTs, but any kind of payload could be handled. The cassette interface provides the low level services to manage the cassette sensors and actuators. It receives the instructions from the cassette monitor and reports the cassette status to it. The ground version of 'EZ_Chains', also called black box embeds the communication system and the knowledge to allow the GCS and the C&C to define the mission and to interact with the fleet. EZ_Chains ground also embeds a tiny mission monitor module doing some checks about the fleet and the mission evolution.","title":"EZ_Chains Fleet Architecture"},{"location":"components/Socs_and_FPGAs/EDI_SoC/","text":"Modular SoC-based embedded reference architecture ID: WP3-02 Contributor: EDI Owner: Licence: Proprietary expected TRL: 4 KET: Contact: rihards.novickis@edi.lv, daniels.justs@edi.lv The currently available embedded drone flight controllers and computational platforms are based on a sequential processing paradigm, which is a limitation to the drone's onboard computational capacity. Modern heterogeneous systems that incorporate different computational paradigms (MPU, FPGA, GPU) promise advantages of great computational capabilities and even better power efficiency with a drawback of greater system complexity. In the COMP4DRONES project, we will utilize our experience in heterogeneous SoC systems to develop an SoC-based embedded reference architecture, which would be the core of the autonomous flight controller. This reference architecture will include a methodology on the algorithm separation between concurrent (digital logic) and sequential processing paradigms, a framework for managing and reusing different software components of the application and component intercommunication framework, including zero data copy communication. Main characteristics: Management of software and hardware components. Distribution of algorithms across heterogeneous processing paradigms. Novel approaches to sensing and processing pipelines. Usage in practice: Access to the developed software / hardware stacks. Access to the documentation and methodologies. Integration of different software and hardware components onto EDI indoor drone.","title":"EDI SoC"},{"location":"components/Socs_and_FPGAs/EDI_SoC/#modular-soc-based-embedded-reference-architecture","text":"ID: WP3-02 Contributor: EDI Owner: Licence: Proprietary expected TRL: 4 KET: Contact: rihards.novickis@edi.lv, daniels.justs@edi.lv The currently available embedded drone flight controllers and computational platforms are based on a sequential processing paradigm, which is a limitation to the drone's onboard computational capacity. Modern heterogeneous systems that incorporate different computational paradigms (MPU, FPGA, GPU) promise advantages of great computational capabilities and even better power efficiency with a drawback of greater system complexity. In the COMP4DRONES project, we will utilize our experience in heterogeneous SoC systems to develop an SoC-based embedded reference architecture, which would be the core of the autonomous flight controller. This reference architecture will include a methodology on the algorithm separation between concurrent (digital logic) and sequential processing paradigms, a framework for managing and reusing different software components of the application and component intercommunication framework, including zero data copy communication. Main characteristics: Management of software and hardware components. Distribution of algorithms across heterogeneous processing paradigms. Novel approaches to sensing and processing pipelines. Usage in practice: Access to the developed software / hardware stacks. Access to the documentation and methodologies. Integration of different software and hardware components onto EDI indoor drone.","title":"Modular\u00a0SoC-based\u00a0embedded reference architecture"},{"location":"components/Socs_and_FPGAs/TUE/","text":"WP3-08_1 and WP3-08_2 A compact, lightweight platform (MPSOC FPGA-based, incl. RTOS) ID: WP3-08* Contributor: TUE Owner: Licence: expected TRL: KET: Contact: hardware Providing a composable and predictable Multi Processor System On Chip (MPSOC) implemented on FPGA. Providing a real-time micro kernel to manage virtualization and resource management in mixed criticality systems. Providing the development and analysis toolchain for implementing real-time systems software on MPSOC. Very accurate real-time interface/library for integration with above platform, Linux, and/or ROS Providing a predictable software framework based on ROS2 for developing real-time software for robotic systems. Modeling and analysis of the real-time behaviour of the robotic systems Proving a real-time messaging framework for robot to robot communication based on Data Distribution Service (DDS) protocol.","title":"TUE - A compact, lightweight platform (MPSOC FPGA-based, incl. RTOS)"},{"location":"components/Socs_and_FPGAs/TUE/#a-compact-lightweight-platform-mpsoc-fpga-based-incl-rtos","text":"ID: WP3-08* Contributor: TUE Owner: Licence: expected TRL: KET: Contact:","title":"A compact, lightweight platform (MPSOC FPGA-based, incl. RTOS)"},{"location":"components/Socs_and_FPGAs/TUE/#hardware","text":"Providing a composable and predictable Multi Processor System On Chip (MPSOC) implemented on FPGA. Providing a real-time micro kernel to manage virtualization and resource management in mixed criticality systems. Providing the development and analysis toolchain for implementing real-time systems software on MPSOC.","title":"hardware"},{"location":"components/Socs_and_FPGAs/TUE/#very-accurate-real-time-interfacelibrary-for-integration-with-above-platform-linux-andor-ros","text":"Providing a predictable software framework based on ROS2 for developing real-time software for robotic systems. Modeling and analysis of the real-time behaviour of the robotic systems Proving a real-time messaging framework for robot to robot communication based on Data Distribution Service (DDS) protocol.","title":"Very accurate real-time interface/library for integration with above platform, Linux, and/or ROS"},{"location":"components/Socs_and_FPGAs/UNIMORE/","text":"Onboard Compute Platform Desing Methodolgy ID: WP3-22 Contributor: UNIMORE Owner: Licence: expected TRL: KET: Contact: Definition of Onboard Compute Platforms (based on commercial off-the-shelf, heterogeneous FPGA SoCs) and methodologies for the deployment of specialized cluster-based, accelerator architecture templates. Compute Platform design should enable easily accessible acceleration for computationally intensive algorithms for Flight Navigation, Positioning, Coordination and Communication. Improvements compare state-of-the-art Onboard Compute Platform are expected in terms of more computational power present on board; a wider set of services and complex algorithms runnable on board; an easy to use deployment methodology for the definition of application-specific accelerated services.","title":"UNIMORE - Onboard Compute Platform Desing Methodolgy"},{"location":"components/Socs_and_FPGAs/UNIMORE/#onboard-compute-platform-desing-methodolgy","text":"ID: WP3-22 Contributor: UNIMORE Owner: Licence: expected TRL: KET: Contact: Definition of Onboard Compute Platforms (based on commercial off-the-shelf, heterogeneous FPGA SoCs) and methodologies for the deployment of specialized cluster-based, accelerator architecture templates. Compute Platform design should enable easily accessible acceleration for computationally intensive algorithms for Flight Navigation, Positioning, Coordination and Communication. Improvements compare state-of-the-art Onboard Compute Platform are expected in terms of more computational power present on board; a wider set of services and complex algorithms runnable on board; an easy to use deployment methodology for the definition of application-specific accelerated services.","title":"Onboard Compute Platform Desing Methodolgy"},{"location":"components/Socs_and_FPGAs/UNISS/","text":"Onboard Compute Platform Design Paradigm ID: WP3-28 Contributor: UNISS Owner: University of Sassari Licence: expected TRL: 4 KET: 2.4.1 - Data Fusion & Processing Contact: (fpalumbo, tfanni)@uniss.it Requirements UC5-FNC-005 : The system SHALL enable advanced on board computation by means of dedicated and optimized accelerators. UC5-FNC-005 : The accelerator shall be built in an energy-and performance-aware manner and, where possible, should support different erformance trade-off and/or functionalities. Specification Onboard computing platforms implementing different working points (i.e. trade-offs among Quality of Service and Energy consumption) or functionalities (i.e. different convolutional kernels). Improvements Extended computational power by leveraging on heterogeneous co-processing units that shall enable advanced and flexible on board computation. Connection with WP4 will lead to integrate this adaptive acceleratores into a more advanced computing infrastructure by integrating it with WP3-22.","title":"UNISS - Onboard Compute Platform Design Paradigm"},{"location":"components/Socs_and_FPGAs/UNISS/#onboard-compute-platform-design-paradigm","text":"ID: WP3-28 Contributor: UNISS Owner: University of Sassari Licence: expected TRL: 4 KET: 2.4.1 - Data Fusion & Processing Contact: (fpalumbo, tfanni)@uniss.it","title":"Onboard Compute Platform Design Paradigm"},{"location":"components/Socs_and_FPGAs/UNISS/#requirements","text":"UC5-FNC-005 : The system SHALL enable advanced on board computation by means of dedicated and optimized accelerators. UC5-FNC-005 : The accelerator shall be built in an energy-and performance-aware manner and, where possible, should support different erformance trade-off and/or functionalities.","title":"Requirements"},{"location":"components/Socs_and_FPGAs/UNISS/#specification","text":"Onboard computing platforms implementing different working points (i.e. trade-offs among Quality of Service and Energy consumption) or functionalities (i.e. different convolutional kernels).","title":"Specification"},{"location":"components/Socs_and_FPGAs/UNISS/#improvements","text":"Extended computational power by leveraging on heterogeneous co-processing units that shall enable advanced and flexible on board computation. Connection with WP4 will lead to integrate this adaptive acceleratores into a more advanced computing infrastructure by integrating it with WP3-22.","title":"Improvements"},{"location":"components/Socs_and_FPGAs/UNIVAQ/","text":"Efficient digital implementation of controllers ID: WP3-24 Contributor: UNIVAQ Owner: Licence: expected TRL: KET: Contact: Efficient digital implementation of controllers, designed on the basis of a discrete\u2013time models, on FPGAs. In order to operate safely, the controller shall enable the functionality of compensating and rejecting environmental perturbations, measurement uncertainties, and possible faults. Increased performance of the controller, with reduced response time. Robustness of the controller with respect to environmental disturbances and increased resiliency.","title":"UNIVAQ - Efficient digital implementation of controllers"},{"location":"components/Socs_and_FPGAs/UNIVAQ/#efficient-digital-implementation-of-controllers","text":"ID: WP3-24 Contributor: UNIVAQ Owner: Licence: expected TRL: KET: Contact: Efficient digital implementation of controllers, designed on the basis of a discrete\u2013time models, on FPGAs. In order to operate safely, the controller shall enable the functionality of compensating and rejecting environmental perturbations, measurement uncertainties, and possible faults. Increased performance of the controller, with reduced response time. Robustness of the controller with respect to environmental disturbances and increased resiliency.","title":"Efficient digital implementation of controllers"},{"location":"components/computer_vision_and_image_processing/BUT/","text":"Sensor information algorithms ID: WP3-03 Contributor: BUT Owner: Licence: expected TRL: KET: Contact: In this component BUT will implement/improve sensor data processing algorithms which will include software and firmware for FPGA. This will involve video processing algorithms (for example HDR algorithms). HDR multi-exposure fusion algorithm to be implemented in the drone, possibly implementing also tone mapping and/or ghost removal (with most probably somewhat liited capabilities) in order to \"feed\" further image and video processing subsystems in the drone by image information with high dynamic range. As a \"demonstrator\", we can also provide e.g. object detection in HDR. Increased performance of the algoritms, which will reduce latency and increase throughput. Robustness of the controller with respect to environmental disturbances and increased resiliency. This improvement will be based on increased robustness of the video processing with respect to HDR while keeping the processing means and extent of video processing \"unchanged\" thanks to the tone mapping that virtually brings the \"same image format\" as in usual processing.","title":"BUT algorithms"},{"location":"components/computer_vision_and_image_processing/BUT/#sensor-information-algorithms","text":"ID: WP3-03 Contributor: BUT Owner: Licence: expected TRL: KET: Contact: In this component BUT will implement/improve sensor data processing algorithms which will include software and firmware for FPGA. This will involve video processing algorithms (for example HDR algorithms). HDR multi-exposure fusion algorithm to be implemented in the drone, possibly implementing also tone mapping and/or ghost removal (with most probably somewhat liited capabilities) in order to \"feed\" further image and video processing subsystems in the drone by image information with high dynamic range. As a \"demonstrator\", we can also provide e.g. object detection in HDR. Increased performance of the algoritms, which will reduce latency and increase throughput. Robustness of the controller with respect to environmental disturbances and increased resiliency. This improvement will be based on increased robustness of the video processing with respect to HDR while keeping the processing means and extent of video processing \"unchanged\" thanks to the tone mapping that virtually brings the \"same image format\" as in usual processing.","title":"Sensor information algorithms"},{"location":"components/computer_vision_and_image_processing/HIB/","text":"Computer Vision Components for drones ID: WP3-04 Contributor: HIB Owner: Licence: expected TRL: KET: Contact: AI system built upon deep learning techniques in order to improve the way of interpreting surroundings and detecting scenarios from data captured with drones AI system built upon automatic algorithms for the autodetection/geo-referencing of road elements and taking cloud of points generated by a LIDAR sensor. Accelerate the Constructive Process of a Civil Infrastructure.","title":"HIB - Computer Vision Components for drones"},{"location":"components/computer_vision_and_image_processing/HIB/#computer-vision-components-for-drones","text":"ID: WP3-04 Contributor: HIB Owner: Licence: expected TRL: KET: Contact: AI system built upon deep learning techniques in order to improve the way of interpreting surroundings and detecting scenarios from data captured with drones AI system built upon automatic algorithms for the autodetection/geo-referencing of road elements and taking cloud of points generated by a LIDAR sensor. Accelerate the Constructive Process of a Civil Infrastructure.","title":"Computer Vision Components for drones"},{"location":"components/computer_vision_and_image_processing/IMEC_1/","text":"Hyperspectral payload ID: WP3-19_1 Contributor: IMEC Owner: Licence: expected TRL: KET: Contact: Hyperspectral cameras can improve detection of material imperfections. The hyperspectral payload will be based on imec\u2019s uav platform: (dual)mosaic sensors/cameras with Ximea break out board and Jetson TX2 board. Regarding the software blocks, we will reuse Airobot\u2019s server based interface for ground controller with imec\u2019s camera commands. The hardware hyperspectral payload is able to communicate with the AiroCore platform. Making a rugged box: hyperspectral measurements for detecting and quantifying corrosion damage on offshore structures in realistic conditions (wind up to 5 Beaufort, wave heights of 1,2-1,5m, minimum temperature of -10\u00b0C).","title":"IMEC - Hyperspectral payload"},{"location":"components/computer_vision_and_image_processing/IMEC_1/#hyperspectral-payload","text":"ID: WP3-19_1 Contributor: IMEC Owner: Licence: expected TRL: KET: Contact: Hyperspectral cameras can improve detection of material imperfections. The hyperspectral payload will be based on imec\u2019s uav platform: (dual)mosaic sensors/cameras with Ximea break out board and Jetson TX2 board. Regarding the software blocks, we will reuse Airobot\u2019s server based interface for ground controller with imec\u2019s camera commands. The hardware hyperspectral payload is able to communicate with the AiroCore platform. Making a rugged box: hyperspectral measurements for detecting and quantifying corrosion damage on offshore structures in realistic conditions (wind up to 5 Beaufort, wave heights of 1,2-1,5m, minimum temperature of -10\u00b0C).","title":"Hyperspectral payload"},{"location":"components/computer_vision_and_image_processing/IMEC_2/","text":"Hyperspectral image processing ID: WP3-19_2 Contributor: IMEC Owner: Licence: expected TRL: KET: Contact: The goal is to use hyperspectral camera data for future navigation : localization and detection. imec will reuse the hyperspectral processing software pipeline as a start base and rely on the in-house built Quasar programming framework to efficiently implement algorithms on the Nvidia Jetson boards. The integration of algorithms to restore hyperspectral images by removing image degradations caused by vibrations, wavelength dependent fading and spectral changes due to lighting conditions. Automatic hyperspectral image based detection and quantification of corrosion using AI technology with an accuracy of 80% compared to human inspections.","title":"IMEC - Hyperspectral image processing"},{"location":"components/computer_vision_and_image_processing/IMEC_2/#hyperspectral-image-processing","text":"ID: WP3-19_2 Contributor: IMEC Owner: Licence: expected TRL: KET: Contact: The goal is to use hyperspectral camera data for future navigation : localization and detection. imec will reuse the hyperspectral processing software pipeline as a start base and rely on the in-house built Quasar programming framework to efficiently implement algorithms on the Nvidia Jetson boards. The integration of algorithms to restore hyperspectral images by removing image degradations caused by vibrations, wavelength dependent fading and spectral changes due to lighting conditions. Automatic hyperspectral image based detection and quantification of corrosion using AI technology with an accuracy of 80% compared to human inspections.","title":"Hyperspectral image processing"},{"location":"components/computer_vision_and_image_processing/INDRA_1/","text":"Visible and Infrared HD camera ID: WP3-27 Contributor: INDRA Owner: Licence: expected TRL: KET: Contact: The camera comes in 3 flavors: a visible spectrum camera, an infrared one, and a dual camera including both visible and infrared spectrum. All cameras are gyro-stabilized on 3 axis. Definitions: Visible: 750x580 Infrared: 640x480 GCS - HMI Human - Machine Interface of the Ground Control Station related to the HD video payload. Analysis, design, development and tests that enable communication between the frontend and the backend of the control station application so that the actions requested by the user, are sent to the autopilot.","title":"INDRA - Payload (Single Visible HD)"},{"location":"components/computer_vision_and_image_processing/INDRA_1/#visible-and-infrared-hd-camera","text":"ID: WP3-27 Contributor: INDRA Owner: Licence: expected TRL: KET: Contact: The camera comes in 3 flavors: a visible spectrum camera, an infrared one, and a dual camera including both visible and infrared spectrum. All cameras are gyro-stabilized on 3 axis. Definitions: Visible: 750x580 Infrared: 640x480","title":"Visible and Infrared HD camera"},{"location":"components/computer_vision_and_image_processing/INDRA_1/#gcs-hmi","text":"Human - Machine Interface of the Ground Control Station related to the HD video payload. Analysis, design, development and tests that enable communication between the frontend and the backend of the control station application so that the actions requested by the user, are sent to the autopilot.","title":"GCS - HMI"},{"location":"components/computer_vision_and_image_processing/SCALIAN-clearance/","text":"EZ_Chains Clearance ID: WP4-5 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 6 KET: 2.2.7 Obstacle Detection and Avoindance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com or mathieu.damour@scalian.com This module aims at providing an automatic method for person detection, vehicles and animals able to do real time analysis of video feeds while embedded on a drone. It is mainly used in the UC3 D1, where a fleet of UAS are dropping sensors. The component returns the respective position in the image of the detected elements and if an element is detected the dropping operation is aborted. The component takes input both from RGB and thermal cameras, the camera can be set in Nadir or oblique view. Since the view point is different, the detection will use different trained models. The component is developped using Tensorflow and Keras for the deep learning. In order to allow real-time detect, a VPU is added to the UAS onboard companion computer. We are using an Intel Neural Compute Stick 2 (NCS2) with the OpenVino framework. Not all instructions available in Tensorflow and Keras can be found in this framework, so some of the operations are kept on the companion computer. The figure below presents the different pipelines available, allowing different detection and outputs: In order to be more generic the component has been develop in the form of a training framework: it is easier to train the system for a new use-case. In order to demonstrate this feature, the component has been trained to detect vehicules on road: N.B.: This model has been not been properly trained, it jsut servesas a demonstrator of the new framework capabilities.","title":"SCALIAN \u2013 Clearance"},{"location":"components/computer_vision_and_image_processing/SCALIAN-clearance/#ez_chains-clearance","text":"ID: WP4-5 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 6 KET: 2.2.7 Obstacle Detection and Avoindance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com or mathieu.damour@scalian.com This module aims at providing an automatic method for person detection, vehicles and animals able to do real time analysis of video feeds while embedded on a drone. It is mainly used in the UC3 D1, where a fleet of UAS are dropping sensors. The component returns the respective position in the image of the detected elements and if an element is detected the dropping operation is aborted. The component takes input both from RGB and thermal cameras, the camera can be set in Nadir or oblique view. Since the view point is different, the detection will use different trained models. The component is developped using Tensorflow and Keras for the deep learning. In order to allow real-time detect, a VPU is added to the UAS onboard companion computer. We are using an Intel Neural Compute Stick 2 (NCS2) with the OpenVino framework. Not all instructions available in Tensorflow and Keras can be found in this framework, so some of the operations are kept on the companion computer. The figure below presents the different pipelines available, allowing different detection and outputs: In order to be more generic the component has been develop in the form of a training framework: it is easier to train the system for a new use-case. In order to demonstrate this feature, the component has been trained to detect vehicules on road: N.B.: This model has been not been properly trained, it jsut servesas a demonstrator of the new framework capabilities.","title":"EZ_Chains Clearance"},{"location":"components/misc/CEA_2/","text":"WP3-32_2 Reference Architecture Modelling and Code Generation The current embedded architectures of drones are organized in loosely coupled monolithic boards. Each board composed of processor, memory and communication resources (e.g. flight control, planning boards). This separation ensures that the subsystems operate almost independently from each other and avoids interference coming from the other parts. However, this approach does not support the continuous development of drone applications such as the ever-increasing demand on autonomy. The COMP4DRONES project will face this challenge by developing a compositional and integrated drone embedded reference architecture following the IMA principles, adapted to, and still considering, the drone resource constraints. With the support of WP6 tools, CEA will enable: The modelling of the compositional and integrated drone embedded reference architecture. The generation of well-formed and semantically-correct ROS code from this architecture.","title":"CEA - Reference Architecture Modelling and Code Generation"},{"location":"components/misc/CEA_2/#reference-architecture-modelling-and-code-generation","text":"The current embedded architectures of drones are organized in loosely coupled monolithic boards. Each board composed of processor, memory and communication resources (e.g. flight control, planning boards). This separation ensures that the subsystems operate almost independently from each other and avoids interference coming from the other parts. However, this approach does not support the continuous development of drone applications such as the ever-increasing demand on autonomy. The COMP4DRONES project will face this challenge by developing a compositional and integrated drone embedded reference architecture following the IMA principles, adapted to, and still considering, the drone resource constraints. With the support of WP6 tools, CEA will enable: The modelling of the compositional and integrated drone embedded reference architecture. The generation of well-formed and semantically-correct ROS code from this architecture.","title":"Reference Architecture Modelling and Code Generation"},{"location":"components/misc/ENSMA_1/","text":"Control components that implement potential barriers ID: WP3-14_1 Contributor: ENSMA Owner: ENSMA Licence: LGPL expected TRL: 4 KET: 1.1.2 - Geofencing, 1.1.6 - Command and Control, 1.3.3 - Detect and Avoid, 2.2.7 - Obstacle Detection and Avoidance, 2.3.2 - Geofencing Contact: patrick.coirault@univ-poitiers.fr Requirements UC3-FNC-002 : The dropping agents shall ensure, with a dedicated software, the clearance of a drop location before dropping a sensor. UC3-PRF-002 : The clearance algorithm shall run on an embedded computer inside the UAVs. It shall not impact the capability of the UAVs to operate their mission. UC3-FNC-05 : SYSTEM shall include an autonomous detect & avoid capability UC3-DEM6-FNC-02 : SYSTEM shall include an autonomous detect & avoid capability Specification A control loop that implements a \"potential field\" that prevents the drone to access certain areas by evaluating its position, geofences and potential obstacles in the environment. It is assumed that drone is equipped with proximity sensors and can detect any relative position of both non-cooperative and cooperative entities within a sensing range. A cooperative entity is another drone which have the same capability. In contrary, a non-cooperative entity is an entity without collision avoidance system. Sensing capability is required to sense the presence of any other entities in its close vicinity which may lead to a collision. These sensors only give the relative position of any entity in its range in the local frame and do not provide position information in the global frame. It is to note that this assumption is used for the purpose of collision avoidance only. Improvements The autonomous flight control includes a mechanism to avoid collision between the drones, obstacles and/or virtual barriers. An Artificial Potential Field (APF) method is used for collision avoidance. In APF, a drone is considered as a point in a potential field. This drone experiences a repulsion force from the obstacles and therefore, instead of colliding with them, it steers around them. Typically, potential functions are based on the relative distance between drone and the obstacle and do not require any global information. Based on the practical aspects, an ideal potential function must have the following properties: The range of the potential field must be bounded. Usually, it depends on the range of obstacle sensors mounted on the agent The value of the potential field and the corresponding repulsion must be infinity at the boundary of the obstacle and must decrease with the increase in the distance First and second derivatives of the potential function must exist in order to have a smooth repulsion force APF based repulsion mechanism is combined with the control algorithm. Interface Inputs: - Distance to the obstacle from sensor proximity detection - Cartesian coordinates and Euler angles of the drone Outputs: - Four rotor speed","title":"ENSMA - Control components that implement potential barriers"},{"location":"components/misc/ENSMA_1/#control-components-that-implement-potential-barriers","text":"ID: WP3-14_1 Contributor: ENSMA Owner: ENSMA Licence: LGPL expected TRL: 4 KET: 1.1.2 - Geofencing, 1.1.6 - Command and Control, 1.3.3 - Detect and Avoid, 2.2.7 - Obstacle Detection and Avoidance, 2.3.2 - Geofencing Contact: patrick.coirault@univ-poitiers.fr","title":"Control components that implement potential barriers"},{"location":"components/misc/ENSMA_1/#requirements","text":"UC3-FNC-002 : The dropping agents shall ensure, with a dedicated software, the clearance of a drop location before dropping a sensor. UC3-PRF-002 : The clearance algorithm shall run on an embedded computer inside the UAVs. It shall not impact the capability of the UAVs to operate their mission. UC3-FNC-05 : SYSTEM shall include an autonomous detect & avoid capability UC3-DEM6-FNC-02 : SYSTEM shall include an autonomous detect & avoid capability","title":"Requirements"},{"location":"components/misc/ENSMA_1/#specification","text":"A control loop that implements a \"potential field\" that prevents the drone to access certain areas by evaluating its position, geofences and potential obstacles in the environment. It is assumed that drone is equipped with proximity sensors and can detect any relative position of both non-cooperative and cooperative entities within a sensing range. A cooperative entity is another drone which have the same capability. In contrary, a non-cooperative entity is an entity without collision avoidance system. Sensing capability is required to sense the presence of any other entities in its close vicinity which may lead to a collision. These sensors only give the relative position of any entity in its range in the local frame and do not provide position information in the global frame. It is to note that this assumption is used for the purpose of collision avoidance only.","title":"Specification"},{"location":"components/misc/ENSMA_1/#improvements","text":"The autonomous flight control includes a mechanism to avoid collision between the drones, obstacles and/or virtual barriers. An Artificial Potential Field (APF) method is used for collision avoidance. In APF, a drone is considered as a point in a potential field. This drone experiences a repulsion force from the obstacles and therefore, instead of colliding with them, it steers around them. Typically, potential functions are based on the relative distance between drone and the obstacle and do not require any global information. Based on the practical aspects, an ideal potential function must have the following properties: The range of the potential field must be bounded. Usually, it depends on the range of obstacle sensors mounted on the agent The value of the potential field and the corresponding repulsion must be infinity at the boundary of the obstacle and must decrease with the increase in the distance First and second derivatives of the potential function must exist in order to have a smooth repulsion force APF based repulsion mechanism is combined with the control algorithm.","title":"Improvements"},{"location":"components/misc/ENSMA_1/#interface","text":"Inputs: - Distance to the obstacle from sensor proximity detection - Cartesian coordinates and Euler angles of the drone Outputs: - Four rotor speed","title":"Interface"},{"location":"components/misc/ENSMA_2/","text":"Multi-agent swarm control ID: WP3-14_2 Contributor: ENSMA Owner: ENSMA Licence: LGPL expected TRL: 3 KET: 1.1.6 - Command and Control, 2.2.7 - Obstacle Detection and Avoidance, 2.5.2 - Swarm Formation and Cooperation Contact: patrick.coirault@univ-poitiers.fr Requirements UC3-OPR-005 : The GCS shall be able to handle multiple agents or system of agents. UC3-OPR-006 : The GCS shall be able to handle different types of agents at the same time. UC3-INT-012 : The GCS shall be able to send commands to agents or system of agents. UC3-FNC-025 : The GCS shall detect trajectory conflicts between different agents Specification In many practical scenarios, it is required that the agents of Multi-Agent System (MAS) create and maintain a desired geometric shape. The required shape could either be fixed or time-varying. In some cases, it is further required that the agents follow a trajectory while maintaining the shape. The trajectory is produced by a virtual or real leader. This is known as formation tracking. Improvements The aim of this task is to develop distributed consensus and formation of a swarm of drones. From the state of art, it is clear that the available cooperative control schemes do not take various limitations in to account. Motivated by this, the current task focused on the design and implementation of distributed cooperative control laws for a swarm of drones with communication and sensor constraints. These considered constraints are given below: Each drone can only measure its position state Drones are not equipped with sensors to measure their velocity Drones do not have access to the input (control) of their neighbors The measured state is transmitted to the neighbors at irregular and non-uniform time intervals The transmission among the drones is asynchronous and totally independent of other drones in the network The communication topology among the drones may be directed or undirected The connectivity of the communication network is maintained Both time-varying and fixed formation shape cases are investigated Since inter-agent collision is another important issue in formation tracking control, a potential function based collision avoidance algorithm is incorporated with the proposed formation tracking controller. The collision avoidance algorithm ensures that the drones converge to produce the desired geometric shape without colliding with each other Interface Inputs: - From communication module, Cartesian coordinates of the neighbors - From internal sensor of the drone, Cartesian coordinates and Euler angles - Distance to the obstacle from sensor proximity detection Outputs: - Four rotor speed","title":"ENSMA - Multi-agent swarm control"},{"location":"components/misc/ENSMA_2/#multi-agent-swarm-control","text":"ID: WP3-14_2 Contributor: ENSMA Owner: ENSMA Licence: LGPL expected TRL: 3 KET: 1.1.6 - Command and Control, 2.2.7 - Obstacle Detection and Avoidance, 2.5.2 - Swarm Formation and Cooperation Contact: patrick.coirault@univ-poitiers.fr","title":"Multi-agent swarm control"},{"location":"components/misc/ENSMA_2/#requirements","text":"UC3-OPR-005 : The GCS shall be able to handle multiple agents or system of agents. UC3-OPR-006 : The GCS shall be able to handle different types of agents at the same time. UC3-INT-012 : The GCS shall be able to send commands to agents or system of agents. UC3-FNC-025 : The GCS shall detect trajectory conflicts between different agents","title":"Requirements"},{"location":"components/misc/ENSMA_2/#specification","text":"In many practical scenarios, it is required that the agents of Multi-Agent System (MAS) create and maintain a desired geometric shape. The required shape could either be fixed or time-varying. In some cases, it is further required that the agents follow a trajectory while maintaining the shape. The trajectory is produced by a virtual or real leader. This is known as formation tracking.","title":"Specification"},{"location":"components/misc/ENSMA_2/#improvements","text":"The aim of this task is to develop distributed consensus and formation of a swarm of drones. From the state of art, it is clear that the available cooperative control schemes do not take various limitations in to account. Motivated by this, the current task focused on the design and implementation of distributed cooperative control laws for a swarm of drones with communication and sensor constraints. These considered constraints are given below: Each drone can only measure its position state Drones are not equipped with sensors to measure their velocity Drones do not have access to the input (control) of their neighbors The measured state is transmitted to the neighbors at irregular and non-uniform time intervals The transmission among the drones is asynchronous and totally independent of other drones in the network The communication topology among the drones may be directed or undirected The connectivity of the communication network is maintained Both time-varying and fixed formation shape cases are investigated Since inter-agent collision is another important issue in formation tracking control, a potential function based collision avoidance algorithm is incorporated with the proposed formation tracking controller. The collision avoidance algorithm ensures that the drones converge to produce the desired geometric shape without colliding with each other","title":"Improvements"},{"location":"components/misc/ENSMA_2/#interface","text":"Inputs: - From communication module, Cartesian coordinates of the neighbors - From internal sensor of the drone, Cartesian coordinates and Euler angles - Distance to the obstacle from sensor proximity detection Outputs: - Four rotor speed","title":"Interface"},{"location":"components/misc/IKERLAN/","text":"WP3-01 Safety function - Pre-Certified SOM ID: WP3-01 Contributor: IKERLAN Owner: Licence: expected TRL: KET: Contact: Analysis of current Safety Standards will be done, and appropriate Safety Concept will be conducted on the development of the HW/SW elements necessary to execute the Safety Function. The Safety function will integrate several sensors to detect obstacles and avoid causing harm to goods or people. As an end point a HW/SW module that implements the Safety Function will be developed. Emergent MPSoC technologies will be used to have powerful computation capabilities. Lidar/radar D&A => update next week","title":"IKERLAN - Safety function - Pre-Certified SOM"},{"location":"components/misc/IKERLAN/#safety-function-pre-certified-som","text":"ID: WP3-01 Contributor: IKERLAN Owner: Licence: expected TRL: KET: Contact: Analysis of current Safety Standards will be done, and appropriate Safety Concept will be conducted on the development of the HW/SW elements necessary to execute the Safety Function. The Safety function will integrate several sensors to detect obstacles and avoid causing harm to goods or people. As an end point a HW/SW module that implements the Safety Function will be developed. Emergent MPSoC technologies will be used to have powerful computation capabilities. Lidar/radar D&A => update next week","title":"Safety function - Pre-Certified SOM"},{"location":"components/misc/SCALIAN-ai_stabilization/","text":"EZ_Chains AI Stabilization ID: WP4-42 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 4 KET: 1.1.6 Command and control, 2.4.1 Data fusion and processing Contact: david.cherel@scalian.com Scalian has experimented with using Deep Reinforcement Learning (DRL) to stabilise UAS flight. The goal is to experiment with the robustness this solution could offer. We train and test our algorithms on a complex drone model. In addition, we take into account the measurement noise and the introduction of disturbances, to test the robustness of the DRL algorithms. To test DRL algorithms, we use the \"OpenAI Baselines\" API available in open-source on GitHub. This API comes with a set of ready-to-use DRL algorithms. It allows researchers to test and improve DRL algorithms on their projects without needing to redevelop everything from scratch. DRL algorithms are generally implemented on simulators developed in a test environment provided by OpenAI. This simulation environment is called \u201cGym\u201d. The developers of OpenAI as well as contributors in this community have provided researchers with \"Gym\"-based simulators for different types of dynamic systems such as the inverted pendulum, the autonomous car, and the drone. These simulators also allow visualization of the behavior of these dynamic systems on a graphical interface while testing DRL algorithms to allow researchers to assess their results. In our work, we used a drone simulator, quadgym, developed in the \"Gym\" environment and based on a more complex mathematical model than the models mentioned in the state of the art. The simulations have shown promising results, so the next step would be to deploy these methods on real drones to assess the quality of these approaches and achieve complete autonomy with as few errors as possible.","title":"SCALIAN \u2013 AI Stabilization"},{"location":"components/misc/SCALIAN-ai_stabilization/#ez_chains-ai-stabilization","text":"ID: WP4-42 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 4 KET: 1.1.6 Command and control, 2.4.1 Data fusion and processing Contact: david.cherel@scalian.com Scalian has experimented with using Deep Reinforcement Learning (DRL) to stabilise UAS flight. The goal is to experiment with the robustness this solution could offer. We train and test our algorithms on a complex drone model. In addition, we take into account the measurement noise and the introduction of disturbances, to test the robustness of the DRL algorithms. To test DRL algorithms, we use the \"OpenAI Baselines\" API available in open-source on GitHub. This API comes with a set of ready-to-use DRL algorithms. It allows researchers to test and improve DRL algorithms on their projects without needing to redevelop everything from scratch. DRL algorithms are generally implemented on simulators developed in a test environment provided by OpenAI. This simulation environment is called \u201cGym\u201d. The developers of OpenAI as well as contributors in this community have provided researchers with \"Gym\"-based simulators for different types of dynamic systems such as the inverted pendulum, the autonomous car, and the drone. These simulators also allow visualization of the behavior of these dynamic systems on a graphical interface while testing DRL algorithms to allow researchers to assess their results. In our work, we used a drone simulator, quadgym, developed in the \"Gym\" environment and based on a more complex mathematical model than the models mentioned in the state of the art. The simulations have shown promising results, so the next step would be to deploy these methods on real drones to assess the quality of these approaches and achieve complete autonomy with as few errors as possible.","title":"EZ_Chains AI Stabilization"},{"location":"components/misc/UDANET_1/","text":"WP3-36_1 Smart and predictive energy management system ID: WP3-36_1 Contributor: UDANET Owner: Licence: expected TRL: KET: Contact: Objective An energy management system is vital to optimize the energy life and the purpose of the system: it will continuously monitor important system parameters, while dealing with the varying power demands of the many aspects, the objectives of the mission and optimizing the usage of the energy. The designed predictive energy management system will be verified and tested via Software in The Loop. Individual technical contribution We seeks to find control inputs and vehicle trajectory between initial and final configurations that minimize the consumed energy during a specific mission. Energetic Model Formulation: drone dynamic model, actuators and battery (or energy souce) dynamic. Optimal energy trajectory planning will be formulated as a minimization problem, by which the final consumed energy is used as the cost function. In addition the state variables and control variables in are constrained to satisfy the vehicle and battery dynamics, boundary conditions. Input The mission data: for example initial and final positions, time interval etc. Output Control inputs that rule the motion and vehicle trajectory to optimize energy consumption","title":"UDANET - Smart and predictive energy management system"},{"location":"components/misc/UDANET_1/#smart-and-predictive-energy-management-system","text":"ID: WP3-36_1 Contributor: UDANET Owner: Licence: expected TRL: KET: Contact:","title":"Smart and predictive energy management system"},{"location":"components/misc/UDANET_1/#objective","text":"An energy management system is vital to optimize the energy life and the purpose of the system: it will continuously monitor important system parameters, while dealing with the varying power demands of the many aspects, the objectives of the mission and optimizing the usage of the energy. The designed predictive energy management system will be verified and tested via Software in The Loop.","title":"Objective"},{"location":"components/misc/UDANET_1/#individual-technical-contribution","text":"We seeks to find control inputs and vehicle trajectory between initial and final configurations that minimize the consumed energy during a specific mission. Energetic Model Formulation: drone dynamic model, actuators and battery (or energy souce) dynamic. Optimal energy trajectory planning will be formulated as a minimization problem, by which the final consumed energy is used as the cost function. In addition the state variables and control variables in are constrained to satisfy the vehicle and battery dynamics, boundary conditions.","title":"Individual technical contribution"},{"location":"components/misc/UDANET_1/#input","text":"The mission data: for example initial and final positions, time interval etc.","title":"Input"},{"location":"components/misc/UDANET_1/#output","text":"Control inputs that rule the motion and vehicle trajectory to optimize energy consumption","title":"Output"},{"location":"components/misc/UDANET_2/","text":"WP3-36_2 AI drone system modules ID: WP3-36_2 Contributor: UDANET Owner: Licence: expected TRL: KET: Contact: Objective Design, training and testing AI algorithms with integrated infrared thermal and camera imaging system to detect and identify parasite animals, to classify leaf diseases. Individual technical contribution The design and implementation of AI methods to classify leaf diseases and detect parasitic animals Analysis and development of the training, verification and testing phases of the algorithms Input Camera data Dataset to train, validate and test the algorithms Output Plant health status classification","title":"UDANET - AI drone system modules"},{"location":"components/misc/UDANET_2/#ai-drone-system-modules","text":"ID: WP3-36_2 Contributor: UDANET Owner: Licence: expected TRL: KET: Contact:","title":"AI drone system modules"},{"location":"components/misc/UDANET_2/#objective","text":"Design, training and testing AI algorithms with integrated infrared thermal and camera imaging system to detect and identify parasite animals, to classify leaf diseases.","title":"Objective"},{"location":"components/misc/UDANET_2/#individual-technical-contribution","text":"The design and implementation of AI methods to classify leaf diseases and detect parasitic animals Analysis and development of the training, verification and testing phases of the algorithms","title":"Individual technical contribution"},{"location":"components/misc/UDANET_2/#input","text":"Camera data Dataset to train, validate and test the algorithms","title":"Input"},{"location":"components/misc/UDANET_2/#output","text":"Plant health status classification","title":"Output"},{"location":"components/misc/UWB/","text":"WP3-26 Droneport: an autonomous drone battery management system ID: WP3-26 Contributor: UWB Owner: Licence: expected TRL: KET: Contact: Droneport (DP) is a complex system for autonomous drone battery management. It can either change battery packs or fast charge small drones. Droneport HW architecture Droneport consists of Drone landing place with landing markers or beacons Battery exchange unit / robotic manipulator Battery management unit for charging and storage Wireless communication to the drone/swarms Optional wired power connection to the drone on the Droneport Droneport SW architecture Droneport SW architecture consists of Drone to DP communication protocol Drone landing assistant Battery exchange system control Charge control Battery management software DP software provides open API for interoperability with various drones flight controllers. PX4 and Mavlink extensions for autonomous drone battery management MAVLink is a very lightweight messaging protocol with hybrid publish-subscribe communication with drones and between onboard drone components. (see https://mavlink.io/en/) PX4 is a complex flight control software widely used in drone community. It performs various basic flight controls and mission tasks. (see https://px4.io) The goal is to create MAVLink extensions via messages specialized for autonomous drone battery management (Drone Port). These extension modules will cover the communication messages prepared for MAVLink library auto generation process. The messages will be subsequently implemented to PX4.","title":"UWB - Autonomous drone battery management"},{"location":"components/misc/UWB/#droneport-an-autonomous-drone-battery-management-system","text":"ID: WP3-26 Contributor: UWB Owner: Licence: expected TRL: KET: Contact: Droneport (DP) is a complex system for autonomous drone battery management. It can either change battery packs or fast charge small drones.","title":"Droneport: an autonomous drone battery management system"},{"location":"components/misc/UWB/#droneport-hw-architecture","text":"Droneport consists of Drone landing place with landing markers or beacons Battery exchange unit / robotic manipulator Battery management unit for charging and storage Wireless communication to the drone/swarms Optional wired power connection to the drone on the Droneport","title":"Droneport HW architecture"},{"location":"components/misc/UWB/#droneport-sw-architecture","text":"Droneport SW architecture consists of Drone to DP communication protocol Drone landing assistant Battery exchange system control Charge control Battery management software DP software provides open API for interoperability with various drones flight controllers.","title":"Droneport SW architecture"},{"location":"components/misc/UWB/#px4-and-mavlink-extensions-for-autonomous-drone-battery-management","text":"MAVLink is a very lightweight messaging protocol with hybrid publish-subscribe communication with drones and between onboard drone components. (see https://mavlink.io/en/) PX4 is a complex flight control software widely used in drone community. It performs various basic flight controls and mission tasks. (see https://px4.io) The goal is to create MAVLink extensions via messages specialized for autonomous drone battery management (Drone Port). These extension modules will cover the communication messages prepared for MAVLink library auto generation process. The messages will be subsequently implemented to PX4.","title":"PX4 and Mavlink extensions for autonomous drone battery management"},{"location":"components/positioning/ACORDE_1/","text":"UWB based indoor positioning ID: WP3-15_1, WP4-17, WP5-19 Contributor: ACORDE Owner: ACORDE Licence: Private expected TRL: 4 KET: Indoor Positioning Contact: fernando.herrera@acorde.com The UltraWideband Indoor Positioning Solution (IPS) of ACORDE will allow an accurate real-time geo-positioning solution within long indoor infrastructures, like tunnels. Solution: The solution consist of a deployment of anchor devices along the indoor infrastructure. Some of them can be fixed, while others can be temporally installed for better accuracy and coverage, i.e. for enabling accurate real-time positioning to a digitization drone flying at specific time slots (e.g., once a week) for Building Information Model (BIM), as required in the construction scenario posed in COMP4DRONES. Out of the infrastructure, at least one external node is used to propagate geo-referencing to both the moving tag and inner anchors. The moving tag fuses UWB-enabled ranges together with its own sensed data (IMU) for smooth, higher data rate position provision to the tag. Improvements: ACORDE IPS presents several distinctive and innovative aspects. ACORDE IPS solution is specifically designed for providing accurate 3D positioning to a single tag. This is crucial for the scenario posed in C4D, where a digitization drone will need safely and accurately navigate within the indoor infrastructure. ACORDE IPS solution considers the challenges coming for positioning on the posed scenario. Long indoor infrastructures pose geometries challenging for beacon based positioning, to ensure accuracy and, at the same time, a cost-effective solution. In addition, the posed solution considers obstacles coming from the own geometry and objects within the infrastructure (e.g. machines). As the digitization scenario is expected to happen at a limited time slot of the indoor infrastructure operation (e.g., 1h a week), ACORDE IPS solution is designed in C4D to be flexible so that it can be reconfigured to provide suited multi-tag positioning and communication services (e.g. for workers, machines) out of the digitization slot (the design of this part is out of C4D scope). Custom Design and Innovation: ACORDE IPS is relying on a custom design of tags and anchors by ACORDE, to support the cost-effective and flexible approach. A novel control access media and application level algorithms to support positioning in the challenged environment are being developed. Moreover, the IPS solution of ACORDE is complemented by a tooling (IPS-MAF) also developed in COMP4DRONES which enables the design of the most effective and efficient deployment. The image below shows a simulation of the dilution of precision suffered for a simple anchor deployment in a simple geometry tunnel. The simulation has been done and visualized with IPS-MAF. It shows that if no care is put on the design of the deployment, accounting geometrical aspects, large degraded accuracy areas easily appear. IPS-MAF is being extended to support this modelling and moreover, other aspects like realistic (and more challenging) geometries, UWB sensor ranges, and static obstacles. For user comfortability, ACORDE IPS solution enables auto-positioning of inner anchors at initialization. At the same time, the solution is flexible to allow the explicit configuration of anchors geo-positioning, to let improve tag position accuracy. Interfaces: TBC (images of HW interfaces, flavour of SW interfaces)","title":"ACORDE - UWB based indoor positioning"},{"location":"components/positioning/ACORDE_1/#uwb-based-indoor-positioning","text":"ID: WP3-15_1, WP4-17, WP5-19 Contributor: ACORDE Owner: ACORDE Licence: Private expected TRL: 4 KET: Indoor Positioning Contact: fernando.herrera@acorde.com The UltraWideband Indoor Positioning Solution (IPS) of ACORDE will allow an accurate real-time geo-positioning solution within long indoor infrastructures, like tunnels. Solution: The solution consist of a deployment of anchor devices along the indoor infrastructure. Some of them can be fixed, while others can be temporally installed for better accuracy and coverage, i.e. for enabling accurate real-time positioning to a digitization drone flying at specific time slots (e.g., once a week) for Building Information Model (BIM), as required in the construction scenario posed in COMP4DRONES. Out of the infrastructure, at least one external node is used to propagate geo-referencing to both the moving tag and inner anchors. The moving tag fuses UWB-enabled ranges together with its own sensed data (IMU) for smooth, higher data rate position provision to the tag. Improvements: ACORDE IPS presents several distinctive and innovative aspects. ACORDE IPS solution is specifically designed for providing accurate 3D positioning to a single tag. This is crucial for the scenario posed in C4D, where a digitization drone will need safely and accurately navigate within the indoor infrastructure. ACORDE IPS solution considers the challenges coming for positioning on the posed scenario. Long indoor infrastructures pose geometries challenging for beacon based positioning, to ensure accuracy and, at the same time, a cost-effective solution. In addition, the posed solution considers obstacles coming from the own geometry and objects within the infrastructure (e.g. machines). As the digitization scenario is expected to happen at a limited time slot of the indoor infrastructure operation (e.g., 1h a week), ACORDE IPS solution is designed in C4D to be flexible so that it can be reconfigured to provide suited multi-tag positioning and communication services (e.g. for workers, machines) out of the digitization slot (the design of this part is out of C4D scope). Custom Design and Innovation: ACORDE IPS is relying on a custom design of tags and anchors by ACORDE, to support the cost-effective and flexible approach. A novel control access media and application level algorithms to support positioning in the challenged environment are being developed. Moreover, the IPS solution of ACORDE is complemented by a tooling (IPS-MAF) also developed in COMP4DRONES which enables the design of the most effective and efficient deployment. The image below shows a simulation of the dilution of precision suffered for a simple anchor deployment in a simple geometry tunnel. The simulation has been done and visualized with IPS-MAF. It shows that if no care is put on the design of the deployment, accounting geometrical aspects, large degraded accuracy areas easily appear. IPS-MAF is being extended to support this modelling and moreover, other aspects like realistic (and more challenging) geometries, UWB sensor ranges, and static obstacles. For user comfortability, ACORDE IPS solution enables auto-positioning of inner anchors at initialization. At the same time, the solution is flexible to allow the explicit configuration of anchors geo-positioning, to let improve tag position accuracy. Interfaces: TBC (images of HW interfaces, flavour of SW interfaces)","title":"UWB based indoor positioning"},{"location":"components/positioning/ACORDE_2/","text":"Multi-antenna GNSS/INS based navigation ID: WP3-15_2, WP4-16, WP5-11 Contributor: ACORDE Owner: ACORDE Licence: Private expected TRL: 5 KET: (Outdoor) Positioning Contact: fernando.herrera@acorde.com In COMP4DRONES, ACORDE develops GLAD+, an improved version of its multi-GNSS Low-cost position and Attitude Determination (GLAD) systems (see ACORDE products and GLAD project site ). (photo courtesy of FADA-CATEC) Solution: GLAD+ provides a trustable, geo-referenced position and attitude based on the fusion of multi-antenna/multi-receiver data and of additional sensors like accelerometers, gyroscopes and barometer. For it, advanced data fusion and navigation algorithms are employed, adapting and exploiting each navigation profile (drone, land-vehicle, etc). Work in C4D focuses on drone profile, a bit more challenging as it prevents some aidings and assumptions usable on other profiles. Improvements: GLAD+ provides a bunch of improvements with regard to GLAD. HW platform is improved while keeping its reduced cost. GLAD+ HW integrates multi-constellation receivers, with and anti-jamming and anti-spoofing capabilities, to provide GLAD+ a qualitative performance upgrade in terms of robustness and integrity. At software level, GLAD+ platform support a main upgrades (new RTOS port, drivers adaptation and enhancements) to keep a highly competitive solution in terms of cost. Moreover, GLAD firmware is being upgraded in several aspects of its internal algorithms for a more robust and accurate attitude estimation. Interfaces: TBC (images of HW interfaces, SDK, Mavlink)","title":"ACORDE - Multi-antenna GNSS/INS based navigation"},{"location":"components/positioning/ACORDE_2/#multi-antenna-gnssins-based-navigation","text":"ID: WP3-15_2, WP4-16, WP5-11 Contributor: ACORDE Owner: ACORDE Licence: Private expected TRL: 5 KET: (Outdoor) Positioning Contact: fernando.herrera@acorde.com In COMP4DRONES, ACORDE develops GLAD+, an improved version of its multi-GNSS Low-cost position and Attitude Determination (GLAD) systems (see ACORDE products and GLAD project site ). (photo courtesy of FADA-CATEC) Solution: GLAD+ provides a trustable, geo-referenced position and attitude based on the fusion of multi-antenna/multi-receiver data and of additional sensors like accelerometers, gyroscopes and barometer. For it, advanced data fusion and navigation algorithms are employed, adapting and exploiting each navigation profile (drone, land-vehicle, etc). Work in C4D focuses on drone profile, a bit more challenging as it prevents some aidings and assumptions usable on other profiles. Improvements: GLAD+ provides a bunch of improvements with regard to GLAD. HW platform is improved while keeping its reduced cost. GLAD+ HW integrates multi-constellation receivers, with and anti-jamming and anti-spoofing capabilities, to provide GLAD+ a qualitative performance upgrade in terms of robustness and integrity. At software level, GLAD+ platform support a main upgrades (new RTOS port, drivers adaptation and enhancements) to keep a highly competitive solution in terms of cost. Moreover, GLAD firmware is being upgraded in several aspects of its internal algorithms for a more robust and accurate attitude estimation. Interfaces: TBC (images of HW interfaces, SDK, Mavlink)","title":"Multi-antenna GNSS/INS based navigation"},{"location":"components/positioning/FADA-CATEC/","text":"WP3-30 Multi-sensor positioning ID: WP3-30 Contributor: FADA-CATEC Owner: Licence: expected TRL: KET: Contact: Storage of the raw data from the LIDAR that provide the environment point cloud Detect obstacles by preventing flight to these obstacles Mapping of the surrounding environment Precision of the navigation in real time less than 1m Integration in ROS operating system, together with the DJI SDK (http://wiki.ros.org/dji_sdk) High level control in order to autonomously navigate in the indoor environment","title":"FADA-CATEC - Indoor positioning module"},{"location":"components/positioning/FADA-CATEC/#multi-sensor-positioning","text":"ID: WP3-30 Contributor: FADA-CATEC Owner: Licence: expected TRL: KET: Contact: Storage of the raw data from the LIDAR that provide the environment point cloud Detect obstacles by preventing flight to these obstacles Mapping of the surrounding environment Precision of the navigation in real time less than 1m Integration in ROS operating system, together with the DJI SDK (http://wiki.ros.org/dji_sdk) High level control in order to autonomously navigate in the indoor environment","title":"Multi-sensor positioning"},{"location":"components/positioning/MODIS/","text":"WP3-20 Multi-sensor positioning ID: WP3-20 Contributor: MODIS Owner: Licence: expected TRL: KET: Contact: Highly embedded customizable platform for SLAM technique, equipped with magnetometers, compasses, GPS (optional), a gyroscope, accelerometer. Additionally the board will be provided with a serial communication bus (or more than one), e.g. USART, SPI, MavLink and a 32-bit MCU. Orientation capabilities through geomagnetic field mapping with similar or higher accuracy to GPS when in GPS-loss navigation and improved precision when GPS is active Improved algorithms for real-time data analytics, and data compression.","title":"MODIS - multi-sensor positioning"},{"location":"components/positioning/MODIS/#multi-sensor-positioning","text":"ID: WP3-20 Contributor: MODIS Owner: Licence: expected TRL: KET: Contact: Highly embedded customizable platform for SLAM technique, equipped with magnetometers, compasses, GPS (optional), a gyroscope, accelerometer. Additionally the board will be provided with a serial communication bus (or more than one), e.g. USART, SPI, MavLink and a 32-bit MCU. Orientation capabilities through geomagnetic field mapping with similar or higher accuracy to GPS when in GPS-loss navigation and improved precision when GPS is active Improved algorithms for real-time data analytics, and data compression.","title":"Multi-sensor positioning"},{"location":"components/positioning/SCALIAN-ez_land/","text":"EZ_Land Precision Landing ID: WP4-2 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 2.3 Positioning, 2.2.1 Take-off, 2.2.2 Landing, 2.2.7 Obstacle Detecton and Avoidance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com The precision landing is a frequent subject for autonomous multicopter because it is an essential component for safety and autonomy in a drone system. Indeed, when relying solely on the GPS signal and IMU data, a drone can have a landing offset up to 5 meters if the signal is bad. Such an offset can be very dangerous: the drone can land on a damaged zone, risking the physical integrity of the drone, or land on a zone with human operators. To demonstrate the efficiency of such a component, the precision landing will be deployed during the METIS use case (UC3-Demo1) to improve the safety of the system and to facilitate the operations of refill and reload of the drone when landed. It will also be deployed during the use case of ATECHSYS (UC3-Demo2) where the multicopter needs to land precisely on a droid TwinswHeel to pick up and drop off a package. Paired up with the clearance component (human detection), the landing will be cancelled if a human is detected near the landing zone, making this component safer. The goal of the component is to ensure a safe , autonomous and precise landing . The component of SCALIAN aims at exploiting the strength of several sensors to ensure a robust, safe and precise landing. To answer the needs of UC3-Demo1 and UC3-Demo2, the sensor IR lock and a computer vision algorithm are integrated in the component. The visual processing algorithm is specific to a design of helipad (on the right). Aruco type markers are not used to have a more robust algorithm. Shadows and creases have a smaller impact on a refined helipad. The component is based on a modular architecture that allows the users to configure which sensors are needed. With this conception, it is also easy to integrate a new type of sensor. It is then possible to fine tune the component to the use-case and its constraints. EZ_Follow - Ground target following EZ_Follow is a mode of EZ_Land, it uses the algorithms to track and follow a ground target (the dedicated dronepad). Thanks to this mode, it is possible to build missions where a UAV take-off from a pad, follows it for a given time and lands back precisely. We have used this mode, on a tethered UAV carrying a camera, to construct a surveillance system very easy and convenient to deploy anywhere: a vehicle carries the pad on its back with the tethered UAV ready to take-off. When arrived where the inspection is required the UAV takes-off, then the vehicle can continue to drive slowly, the UAV will follow it. During the phase, the camera operator (or security personnel) can focus only on its mission, the inspection. Indeed the UAV maintains its altitude and position relative to the vehicle. When the mission is done, the UAV lands precisely on the back of the vehicle (the precision ensures that it will not fall off), finally the vehicle can move to the next inspection area. This system is called Long-Eye .","title":"SCALIAN \u2013 EZ_land Precision Landing"},{"location":"components/positioning/SCALIAN-ez_land/#ez_land-precision-landing","text":"ID: WP4-2 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 2.3 Positioning, 2.2.1 Take-off, 2.2.2 Landing, 2.2.7 Obstacle Detecton and Avoidance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com The precision landing is a frequent subject for autonomous multicopter because it is an essential component for safety and autonomy in a drone system. Indeed, when relying solely on the GPS signal and IMU data, a drone can have a landing offset up to 5 meters if the signal is bad. Such an offset can be very dangerous: the drone can land on a damaged zone, risking the physical integrity of the drone, or land on a zone with human operators. To demonstrate the efficiency of such a component, the precision landing will be deployed during the METIS use case (UC3-Demo1) to improve the safety of the system and to facilitate the operations of refill and reload of the drone when landed. It will also be deployed during the use case of ATECHSYS (UC3-Demo2) where the multicopter needs to land precisely on a droid TwinswHeel to pick up and drop off a package. Paired up with the clearance component (human detection), the landing will be cancelled if a human is detected near the landing zone, making this component safer. The goal of the component is to ensure a safe , autonomous and precise landing . The component of SCALIAN aims at exploiting the strength of several sensors to ensure a robust, safe and precise landing. To answer the needs of UC3-Demo1 and UC3-Demo2, the sensor IR lock and a computer vision algorithm are integrated in the component. The visual processing algorithm is specific to a design of helipad (on the right). Aruco type markers are not used to have a more robust algorithm. Shadows and creases have a smaller impact on a refined helipad. The component is based on a modular architecture that allows the users to configure which sensors are needed. With this conception, it is also easy to integrate a new type of sensor. It is then possible to fine tune the component to the use-case and its constraints.","title":"EZ_Land Precision Landing"},{"location":"components/positioning/SCALIAN-ez_land/#ez_follow-ground-target-following","text":"EZ_Follow is a mode of EZ_Land, it uses the algorithms to track and follow a ground target (the dedicated dronepad). Thanks to this mode, it is possible to build missions where a UAV take-off from a pad, follows it for a given time and lands back precisely. We have used this mode, on a tethered UAV carrying a camera, to construct a surveillance system very easy and convenient to deploy anywhere: a vehicle carries the pad on its back with the tethered UAV ready to take-off. When arrived where the inspection is required the UAV takes-off, then the vehicle can continue to drive slowly, the UAV will follow it. During the phase, the camera operator (or security personnel) can focus only on its mission, the inspection. Indeed the UAV maintains its altitude and position relative to the vehicle. When the mission is done, the UAV lands precisely on the back of the vehicle (the precision ensures that it will not fall off), finally the vehicle can move to the next inspection area. This system is called Long-Eye .","title":"EZ_Follow - Ground target following"},{"location":"components/secure_communications/CEA/","text":"WP3-32_1 TSN Queue Mapper ID: WP3-32_1 Contributor: CEA Owner: Licence: expected TRL: KET: Contact: On the drone, it is expected that communications between different components could be supported by a TSN Network (Time-Sensitive Network). TSN is a group of IEEE Standards that targets support of deterministic communications over standard Ethernet. Several traffic Queues can be defined to support different levels of TSN support (determinism, controlled latency, best efforts, etc.). This software is in charge of setting up the TSN queues and the routing rules so that Traffic with specific QoS requirements can be handled as expected in the TSN network (on-board). This software is required to setup the TSN flows.","title":"CEA - TSN Queue Mapper"},{"location":"components/secure_communications/CEA/#tsn-queue-mapper","text":"ID: WP3-32_1 Contributor: CEA Owner: Licence: expected TRL: KET: Contact: On the drone, it is expected that communications between different components could be supported by a TSN Network (Time-Sensitive Network). TSN is a group of IEEE Standards that targets support of deterministic communications over standard Ethernet. Several traffic Queues can be defined to support different levels of TSN support (determinism, controlled latency, best efforts, etc.). This software is in charge of setting up the TSN queues and the routing rules so that Traffic with specific QoS requirements can be handled as expected in the TSN network (on-board). This software is required to setup the TSN flows.","title":"TSN Queue Mapper"},{"location":"components/secure_communications/IFAT/","text":"WP3-10 Component for trusted communication ID: WP3-10 Contributor: IFAT Owner: Licence: expected TRL: KET: Contact: IFAT is developing a hardware-security-component (WP5), typically denoted as \"Secure Element\", and corresponding generic Software-Libraries (WP3). Both parts (HW+SW) combined will be then prepared (as reference-demo) to be integrated by future drone-system-integration partners to raise the overall trustworthiness of their (maybe already existing) communication sub-architecture. Therefore, this component is seen as one important sub-component contributing to an overall \"trusted communication architecture\" . Shall ensure confidentiality, authenticity and integrity of credentials. Primary functional focus of this component is: the protection of the most critical credentials (e.g. private keys of a PKI) in a temper-resistant storage (to prevent potential extraction if an attacker would obtain physical access to a drone) performing essential cryptographic operations during security-critical communication handshake-procedures (e.g. TLS-handshake) in a hardware-secured environment to finally achieve secured mutual-authentication of the drone towards various remote stations (e.g. flight control server, or e.g. receiver of camera-date, etc). Remark The purpose of this security-component should not be confused with the authentication of the drone towards the mobile network operator (MNO) for network access. That is usually already done today using standard SIM/eSIM hardware (which is not the purpose of the component developed by IFAT in the course this project).","title":"IFAT - Component for trusted communication"},{"location":"components/secure_communications/IFAT/#component-for-trusted-communication","text":"ID: WP3-10 Contributor: IFAT Owner: Licence: expected TRL: KET: Contact: IFAT is developing a hardware-security-component (WP5), typically denoted as \"Secure Element\", and corresponding generic Software-Libraries (WP3). Both parts (HW+SW) combined will be then prepared (as reference-demo) to be integrated by future drone-system-integration partners to raise the overall trustworthiness of their (maybe already existing) communication sub-architecture. Therefore, this component is seen as one important sub-component contributing to an overall \"trusted communication architecture\" . Shall ensure confidentiality, authenticity and integrity of credentials. Primary functional focus of this component is: the protection of the most critical credentials (e.g. private keys of a PKI) in a temper-resistant storage (to prevent potential extraction if an attacker would obtain physical access to a drone) performing essential cryptographic operations during security-critical communication handshake-procedures (e.g. TLS-handshake) in a hardware-secured environment to finally achieve secured mutual-authentication of the drone towards various remote stations (e.g. flight control server, or e.g. receiver of camera-date, etc). Remark The purpose of this security-component should not be confused with the authentication of the drone towards the mobile network operator (MNO) for network access. That is usually already done today using standard SIM/eSIM hardware (which is not the purpose of the component developed by IFAT in the course this project).","title":"Component for trusted communication"},{"location":"components/secure_communications/SCALIAN-safe_fleet_comm/","text":"EZ_Com Safe fleet communication ID: WP5-03-SCALIAN Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 1.3.1 Vehicle to Vehicle communication, 1.3.2 Vehicle to Infrastructure communication, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com : Scalian has worked on developing a generic architecture to allow fleet of UAVs or miscellaneous agents to perform a variety of missions. This architecture is composed of several components and a Knowledge Base whose role is to store information on the mission status and UAVs status. In a system like this, the communication system is very important to provide all the information of each agent to each other. This module is the abstraction of the communication to ensure that the agents KB are correctly sychronised. On the hardware part of the communication system, Scalian has decided to use a 4G private network. However this solution is expensive and require operations (deployement of antenna, and so on) prior to any UAV operations. So Scalian aims at creating a component for safe fleet communcation along two axis: Use 4G public network, and setup a Virtual Private Netork (VPN) to ensure a proper level of safety and security. Additionnaly, dedicated services will be configured: e.g. video stream. Replace the 4G network with a new communication mean. A solution could be a mesh network. Both axis will be sutdied in parallel and could require to be merged into one, depending on the results obtained during the C4D project.","title":"SCALIAN \u2013 AI Stabilization"},{"location":"components/secure_communications/SCALIAN-safe_fleet_comm/#ez_com-safe-fleet-communication","text":"ID: WP5-03-SCALIAN Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 1.3.1 Vehicle to Vehicle communication, 1.3.2 Vehicle to Infrastructure communication, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com : Scalian has worked on developing a generic architecture to allow fleet of UAVs or miscellaneous agents to perform a variety of missions. This architecture is composed of several components and a Knowledge Base whose role is to store information on the mission status and UAVs status. In a system like this, the communication system is very important to provide all the information of each agent to each other. This module is the abstraction of the communication to ensure that the agents KB are correctly sychronised. On the hardware part of the communication system, Scalian has decided to use a 4G private network. However this solution is expensive and require operations (deployement of antenna, and so on) prior to any UAV operations. So Scalian aims at creating a component for safe fleet communcation along two axis: Use 4G public network, and setup a Virtual Private Netork (VPN) to ensure a proper level of safety and security. Additionnaly, dedicated services will be configured: e.g. video stream. Replace the 4G network with a new communication mean. A solution could be a mesh network. Both axis will be sutdied in parallel and could require to be merged into one, depending on the results obtained during the C4D project.","title":"EZ_Com Safe fleet communication"},{"location":"components/secure_communications/TNL/","text":"WP3-07 Robust Communication ID: WP3-07 Contributor: TNL Owner: Licence: expected TRL: KET: Contact: Robust communications by means of store- and forwarding methods, using mechanisms from Disruption Tolerant Networking (DTN). Focus is on collection of sensors observations in areas where standard connectivity may be limited. Robust interrupt and tolerant communication system. Study of components, methods algorithms, leading to a small-footprint implementation. Specific focus: quality of service methods (priority classes), routing algorithms (in drone context) (static, epidemic, spray-and-wait) and the possibility to create the functionality in embedded systems with small footprint","title":"TNL - Robust Communication"},{"location":"components/secure_communications/TNL/#robust-communication","text":"ID: WP3-07 Contributor: TNL Owner: Licence: expected TRL: KET: Contact: Robust communications by means of store- and forwarding methods, using mechanisms from Disruption Tolerant Networking (DTN). Focus is on collection of sensors observations in areas where standard connectivity may be limited. Robust interrupt and tolerant communication system. Study of components, methods algorithms, leading to a small-footprint implementation. Specific focus: quality of service methods (priority classes), routing algorithms (in drone context) (static, epidemic, spray-and-wait) and the possibility to create the functionality in embedded systems with small footprint","title":"Robust Communication"}]}